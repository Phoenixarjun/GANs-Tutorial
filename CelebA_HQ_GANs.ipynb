{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction**\n",
        "\n",
        "> Imagine giving a blank canvas and a paintbrush to a rookie artist who has never drawn before. Every day, this artist tries to sketch faces. A tough critic watches over them, pointing out flaws relentlessly. Slowly, the artist improves, until one day, their portraits are indistinguishable from real photographs.\n",
        "\n",
        "That‚Äôs the magic of **GANs (Generative Adversarial Networks)**.\n",
        "\n",
        "* **Generator** ‚Üí the rookie artist (creates faces from random noise).\n",
        "* **Discriminator** ‚Üí the art critic (judges real vs fake).\n",
        "\n",
        "In this notebook, we‚Äôll train a **Deep Convolutional GAN (DCGAN)** on the **CelebA-HQ dataset** (30k+ high-quality celebrity faces, resized to 256x256). By the end, you‚Äôll see an AI ‚Äúlearn‚Äù to paint realistic celebrity portraits pixel by pixel.\n"
      ],
      "metadata": {
        "id": "0s3W5Z3WSLUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **2. Setup**\n",
        "\n",
        "Let‚Äôs import the essentials and grab the dataset."
      ],
      "metadata": {
        "id": "AEkm4tx6SZJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi6FWZYKOjLI"
      },
      "outputs": [],
      "source": [
        "project_name = 'CelebA-HQ'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ],
      "metadata": {
        "id": "0dvoRHRQPJlR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256'\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "id": "PTkJ_JhYPLXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "DATA_DIR = './celebahq-resized-256x256'\n",
        "print(os.listdir(DATA_DIR))"
      ],
      "metadata": {
        "id": "Q9ziPo-8PWqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(DATA_DIR+'/celeba_hq_256')[:10])"
      ],
      "metadata": {
        "id": "cMufzu54RcmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid"
      ],
      "metadata": {
        "id": "-B5C5ZPCRgmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è Preparing Our Celebrity Portraits\n",
        "\n",
        "Before our rookie artist (the generator) can start painting, we need to **set up the reference gallery** of real celebrity portraits. But we can‚Äôt just throw in random photos we need them **uniform, standardized, and ready for practice**.\n",
        "\n",
        "Here‚Äôs what we do:\n",
        "\n",
        "1. **Set the stage**"
      ],
      "metadata": {
        "id": "vKrSUNvuTYyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 256\n",
        "batch_size = 32\n",
        "\n",
        "stats = (0.5,0.5,0.5), (0.5,0.5,0.5)"
      ],
      "metadata": {
        "id": "q5HrZ4BxRjUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * `image_size=256` ‚Üí all faces are resized to **256√ó256**, like giving every canvas the same dimensions.\n",
        "   * `batch_size=32` ‚Üí the number of portraits shown to the artist in one round.\n",
        "   * `stats` ‚Üí scales pixels from `[0,1]` to `[-1,1]`. This matches the generator‚Äôs **Tanh output**, keeping learning stable."
      ],
      "metadata": {
        "id": "wirNNGBlTe1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Load & Transform the portraits**"
      ],
      "metadata": {
        "id": "qXkA0LeuTjrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing pipeline\n",
        "train_ds = ImageFolder(\n",
        "    root=DATA_DIR,\n",
        "    transform=T.Compose([\n",
        "        T.Resize(image_size),\n",
        "        T.CenterCrop(image_size),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(*stats)\n",
        "    ])\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "train_dl = DataLoader(\n",
        "    train_ds, batch_size=batch_size,\n",
        "    shuffle=True, num_workers=3, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "KPrObUtkTKIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * `ImageFolder` ‚Üí tells PyTorch: *‚Äútreat this folder of images as a dataset.‚Äù*\n",
        "   * `Resize` + `CenterCrop` ‚Üí ensure every face is the same size and centered (like cropping portraits to focus on the face).\n",
        "   * `ToTensor` ‚Üí converts images into tensors (the language our AI understands).\n",
        "   * `Normalize` ‚Üí rescales colors so the generator and discriminator speak the same ‚Äúpixel language.‚Äù\n",
        "   * `DataLoader` ‚Üí creates a conveyor belt that hands the critic and artist mini-batches of images, shuffled every epoch for variety.\n"
      ],
      "metadata": {
        "id": "8Uhq7b5jTjCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "dEjue3trTofJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Teaching Text for the Two Cells\n",
        "\n",
        "**1. Denormalization Function** :\n",
        "When training GANs, we normalize images to the range `[-1, 1]` (using `T.Normalize`). But when we want to **see the results** (plot the images), we need to ‚Äúundo‚Äù that normalization.\n",
        "\n",
        "üëâ This `denorm` function rescales the tensors back to the original `[0, 1]` range so matplotlib can display them properly."
      ],
      "metadata": {
        "id": "u0qb6WgBUH1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0]"
      ],
      "metadata": {
        "id": "dk1aVbGBUKSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Visualization Utilities**\n",
        "\n",
        "\n",
        "* `show_images`: takes a batch of tensors, denormalizes them, arranges them into a neat grid (`make_grid`), and shows them.\n",
        "* `detach()`: makes sure gradients aren‚Äôt tracked (since we‚Äôre only visualizing).\n",
        "* `permute(1,2,0)`: reorders tensor dimensions from PyTorch format `(C,H,W)` to matplotlib format `(H,W,C)`.\n",
        "\n",
        "\n",
        "* `show_batch`: grabs **one batch** from the dataloader (`dl`) and calls `show_images`.\n",
        "* Super handy to quickly **preview what your dataset looks like after preprocessing** (resize, crop, normalize).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UrfAu1TtUV1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, nmax=64):\n",
        "  fig, ax = plt.subplots(figsize=(8,8))\n",
        "  ax.set_xticks([]); ax.set_yticks([])\n",
        "  ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1,2,0))\n",
        "\n",
        "\n",
        "def show_batch(dl, nmax=64):\n",
        "  for images, _ in dl:\n",
        "    show_images(images, nmax)\n",
        "    break"
      ],
      "metadata": {
        "id": "c1rJ-vIRUTD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Together, these functions act like your **GAN microscope**  they let you peek at your dataset and later your generator‚Äôs output, ensuring everything is working as intended."
      ],
      "metadata": {
        "id": "cGBfgpJ_UcW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_batch(train_dl)"
      ],
      "metadata": {
        "id": "r9Jan7yHTqQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block makes training **device-agnostic and GPU-ready**:\n",
        "\n",
        "1. **`get_default_device`** ‚Üí picks GPU (`cuda`) if available, else CPU.\n",
        "2. **`to_device`** ‚Üí moves tensors (or lists/tuples) onto the chosen device efficiently.\n",
        "3. **`DeviceDataLoader`** ‚Üí wraps a PyTorch DataLoader so every batch is automatically pushed to the right device.\n",
        "\n",
        "‚úÖ Together, they remove the hassle of calling `.to(device)` everywhere and let the training loop run seamlessly on GPU or CPU.\n"
      ],
      "metadata": {
        "id": "I920eVDpU1_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "  if isinstance(data, (list, tuple)):\n",
        "    return [to_device(x, device) for x in data]\n",
        "  return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl = dl\n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dl)"
      ],
      "metadata": {
        "id": "uzD7qnk3U2Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "id": "8P_Vv7BjVFSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)"
      ],
      "metadata": {
        "id": "BTCA0FelVGn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ This wraps the dataloader so every batch is automatically moved to the GPU (CUDA) for faster training."
      ],
      "metadata": {
        "id": "oH-YSfcFVW2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "byoRWWgnVIBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = nn.Sequential(\n",
        "    # in: 3 x 256 x 256\n",
        "\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 64 x 128 x 128\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 128 x 64 x 64\n",
        "\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 256 x 32 x 32\n",
        "\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 512 x 16 x 16\n",
        "\n",
        "    nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(1024),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 1024 x 8 x 8\n",
        "\n",
        "    nn.Conv2d(1024, 2048, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(2048),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 2048 x 4 x 4\n",
        "\n",
        "    nn.Conv2d(2048, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    # out: 1 x 1 x 1\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "6CWQKkpjVrSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the **Discriminator for CelebA-HQ at 256√ó256 resolution**.\n",
        "\n",
        "* The input is a **3√ó256√ó256 RGB image**.\n",
        "* Each block is **Conv2D ‚Üí BatchNorm ‚Üí LeakyReLU**:\n",
        "\n",
        "  * Convolution downsamples the image (halves width & height).\n",
        "  * BatchNorm stabilizes training.\n",
        "  * LeakyReLU introduces non-linearity and avoids ‚Äúdying ReLUs.‚Äù\n",
        "* With each step, the **spatial resolution shrinks** (`256 ‚Üí 128 ‚Üí 64 ‚Üí ‚Ä¶ ‚Üí 4`), while **channels grow** (`3 ‚Üí 64 ‚Üí ‚Ä¶ ‚Üí 2048`).\n",
        "* Finally, a `1√ó1` feature map is reduced to a **single scalar**, passed through `Sigmoid()` ‚Üí giving the probability that the image is **real (close to 1)** or **fake (close to 0)**.\n",
        "\n",
        "‚úÖ In essence: The Discriminator acts as a **deep CNN classifier**, trained to be a **lie detector** for your GAN.\n"
      ],
      "metadata": {
        "id": "VyvzXGyUVur5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = to_device(discriminator, device)"
      ],
      "metadata": {
        "id": "vxnbdXXmVxgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ We move the discriminator to the GPU (CUDA) for faster training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSJB5EsrV7Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_size = 128\n",
        "\n",
        "generator = nn.Sequential(\n",
        "    # in: latent_size x 1 x 1\n",
        "\n",
        "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(True),\n",
        "    # out: 512 x 4 x 4\n",
        "\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(True),\n",
        "    # out: 256 x 8 x 8\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(True),\n",
        "    # out: 128 x 16 x 16\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(True),\n",
        "    # out: 64 x 32 x 32\n",
        "\n",
        "    nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(True),\n",
        "    # out: 32 x 64 x 64\n",
        "\n",
        "    nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(True),\n",
        "    # out: 16 x 128 x 128\n",
        "\n",
        "    nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh()\n",
        "    # out: 3 x 256 x 256\n",
        ")"
      ],
      "metadata": {
        "id": "WHrHVMn_WHPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **generator** takes a random noise vector (`latent_size = 128`) and transforms it step by step into a **256√ó256 RGB image**.\n",
        "\n",
        "* Each `ConvTranspose2d` layer acts like an ‚Äúupscaling block,‚Äù gradually doubling the image size (4√ó4 ‚Üí 8√ó8 ‚Üí ‚Ä¶ ‚Üí 256√ó256).\n",
        "* `BatchNorm2d` stabilizes training.\n",
        "* `ReLU` introduces non-linearity and helps the network learn richer features.\n",
        "* Finally, `Tanh` squashes the pixel values to `[-1, 1]`, matching the normalization of our dataset.\n",
        "\n",
        "üëâ In short: the generator is like an ‚Äúartist‚Äù starting with random noise and progressively painting a high-resolution face.\n",
        "\n",
        "---\n",
        "\n",
        "‚ú® **Bonus ‚Äî Understanding CNN parameters**:\n",
        "\n",
        "* **Kernel size** = how big my magnifying glass is.\n",
        "* **Stride** = how far I move the magnifying glass each step.\n",
        "* **Padding** = do I add a border so the edges also get looked at properly?\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4osIFoOBWgcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xb = torch.randn(batch_size, latent_size, 1, 1)\n",
        "fake_images = generator(xb)\n",
        "print(fake_images.shape)\n",
        "show_images(fake_images)"
      ],
      "metadata": {
        "id": "lk0iBmLUWip_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = to_device(generator, device)"
      ],
      "metadata": {
        "id": "iU_UolKCXh4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ We move the generator to the GPU (CUDA) for faster training."
      ],
      "metadata": {
        "id": "3l8lIkmmdEdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_discriminator(real_images, opt_d):\n",
        "    # real_images: batch of real face images from CelebA\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    # 1. Pass real CelebA faces through discriminator\n",
        "    real_preds = discriminator(real_images)\n",
        "    real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
        "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "    real_score = torch.mean(real_preds).item()\n",
        "\n",
        "    # 2. Generate fake faces using generator\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    fake_images = generator(latent)\n",
        "\n",
        "    # 3. Pass generated faces through discriminator\n",
        "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
        "    fake_preds = discriminator(fake_images.detach())   # detach to avoid backprop to generator\n",
        "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
        "    fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "    # 4. Update discriminator: maximize real/fake separation\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "\n",
        "    return loss.item(), real_score, fake_score"
      ],
      "metadata": {
        "id": "gv30haUCdG2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **discriminator** acts like an art critic for faces. Its job is to separate **real celebrity faces** from **fake ones generated by the generator**.\n",
        "\n",
        "The process happens in four steps:\n",
        "\n",
        "1. **Judge Real Faces**\n",
        "\n",
        "   * Real images from the **CelebA dataset** are fed into the discriminator.\n",
        "   * The target label is **1 (real)**.\n",
        "   * Loss is computed: how well did it recognize real faces?\n",
        "\n",
        "2. **Judge Fake Faces**\n",
        "\n",
        "   * A random noise vector is passed to the generator to create **fake faces**.\n",
        "   * These fakes are shown to the discriminator.\n",
        "   * The target label is **0 (fake)**.\n",
        "   * Loss is computed: how well did it catch the fakes?\n",
        "\n",
        "3. **Combine Losses**\n",
        "\n",
        "   * Total loss = loss on real + loss on fake.\n",
        "   * This ensures the critic balances both tasks: praising the real and rejecting the fake.\n",
        "\n",
        "4. **Update the Critic**\n",
        "\n",
        "   * Backpropagation updates the discriminator‚Äôs weights.\n",
        "   * Each step makes it a sharper critic, harder to fool.\n",
        "\n",
        "---\n",
        "\n",
        "‚ö° **Key Idea:**\n",
        "\n",
        "* The discriminator doesn‚Äôt just ‚Äúlearn faces.‚Äù\n",
        "* It learns the **essence of what makes a face real** vs. a flawed fake.\n",
        "* As it improves, the generator is forced to get better at creating realistic CelebA-like faces.\n",
        "\n"
      ],
      "metadata": {
        "id": "CXIkCQSZdj8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator(opt_g):\n",
        "    # Generator‚Äôs job: create fake CelebA-like faces good enough to fool the critic\n",
        "    opt_g.zero_grad()\n",
        "\n",
        "    # 1. Generate fake faces from random noise\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
        "    fake_images = generator(latent)\n",
        "\n",
        "    # 2. Discriminator evaluates the fake faces\n",
        "    preds = discriminator(fake_images)\n",
        "\n",
        "    # 3. Trick the critic: pretend fakes are real (target = 1)\n",
        "    targets = torch.ones(batch_size, 1, device=device)\n",
        "    loss = F.binary_cross_entropy(preds, targets)\n",
        "\n",
        "    # 4. Update generator: push it to make more realistic CelebA faces\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "zR-Szw0OdnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **generator** is like a *forger* trying to paint fake CelebA faces that look real enough to trick the critic (discriminator).\n",
        "\n",
        "**Step-by-step logic:**\n",
        "\n",
        "1. üé≤ **Noise in, face out**\n",
        "   We start with pure randomness (latent vector) and let the generator convert it into a batch of fake faces.\n",
        "\n",
        "2. üïµÔ∏è **Critic‚Äôs judgment**\n",
        "   These fake faces are passed into the discriminator, which tries to classify them as fake.\n",
        "\n",
        "3. üé≠ **Fooling strategy**\n",
        "   Instead of aiming for `0` (fake), we assign the label `1` (real).\n",
        "   ‚Üí The generator is literally ‚Äúlying‚Äù to the critic, trying to maximize the chance that fake faces are accepted as real.\n",
        "\n",
        "4. ‚ö° **Learning update**\n",
        "   The loss is backpropagated, and the generator‚Äôs weights are adjusted so its next batch of fake CelebA faces looks more realistic.\n",
        "\n",
        "üëâ In short: *The generator gets better at painting faces by constantly trying to deceive the discriminator.*\n"
      ],
      "metadata": {
        "id": "5K3cYb5rd8qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "YNDtPIhkd-e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dir = 'generated'\n",
        "os.makedirs(sample_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "2SlQiPcoeFUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a generated folder (if it doesn‚Äôt exist) to save the images produced by our generator."
      ],
      "metadata": {
        "id": "vGldih4BeVdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_samples(index, latent_tensors, show=True):\n",
        "    # Generate fake images\n",
        "    fake_images = generator(latent_tensors).detach()\n",
        "\n",
        "    # Save to disk\n",
        "    fake_fname = f'generated-images-{index:04d}.png'\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
        "    print(\"Saving\", fake_fname)\n",
        "\n",
        "    # Optionally display\n",
        "    if show:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(denorm(fake_images).cpu(), nrow=8).permute(1, 2, 0))\n"
      ],
      "metadata": {
        "id": "ipvoKDwfeGZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
      ],
      "metadata": {
        "id": "V8gjeQ75ehMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_samples(0, fixed_latent)"
      ],
      "metadata": {
        "id": "OZ55w-Qjet_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "pm5UIoqLeyz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(epochs, lr, start_idx=1):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Losses & scores\n",
        "    losses_g = []\n",
        "    losses_d = []\n",
        "    real_scores = []\n",
        "    fake_scores = []\n",
        "\n",
        "    # Create optimizers\n",
        "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for real_images, _ in tqdm(train_dl, leave=False):\n",
        "            # Train discriminator\n",
        "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
        "            # Train generator\n",
        "            loss_g = train_generator(opt_g)\n",
        "\n",
        "        # Record losses & scores\n",
        "        losses_g.append(loss_g)\n",
        "        losses_d.append(loss_d)\n",
        "        real_scores.append(real_score)\n",
        "        fake_scores.append(fake_score)\n",
        "\n",
        "        # Log losses & scores (last batch)\n",
        "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "\n",
        "        # Save generated images\n",
        "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
        "\n",
        "    return losses_g, losses_d, real_scores, fake_scores"
      ],
      "metadata": {
        "id": "TJWOBJcee3Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function runs the **GAN training process** on CelebA.\n",
        "\n",
        "1. **Setup:**\n",
        "\n",
        "   * Clears GPU cache.\n",
        "   * Initializes Adam optimizers for both discriminator & generator.\n",
        "   * Prepares lists to track losses and scores.\n",
        "\n",
        "2. **Epoch loop:**\n",
        "\n",
        "   * For each batch of CelebA images:\n",
        "\n",
        "     * üéØ **Discriminator** learns to separate real CelebA faces from fake ones.\n",
        "     * üé® **Generator** learns to fool the discriminator with better fakes.\n",
        "\n",
        "3. **Tracking:**\n",
        "\n",
        "   * Records generator loss, discriminator loss, and average scores for real vs fake images.\n",
        "   * Logs progress each epoch.\n",
        "\n",
        "4. **Visualization:**\n",
        "\n",
        "   * Saves generated samples using a fixed noise vector so we can **see improvements over time**.\n",
        "\n",
        "üëâ In short: this is the **engine room** of training  where the generator and discriminator continuously battle and improve, epoch by epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "lo-rTy1jfNGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0002\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "j3e2_7BmfPQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(epochs, lr)"
      ],
      "metadata": {
        "id": "Mpci5EGJfc9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoints\n",
        "torch.save(generator.state_dict(), 'G.ckpt')\n",
        "torch.save(discriminator.state_dict(), 'D.ckpt')"
      ],
      "metadata": {
        "id": "6Kq4CfsAgkei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "iTdcdScpglr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('./generated/generated-images-0001.png')"
      ],
      "metadata": {
        "id": "62IWTeoxgms1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "vid_fname = 'gans_training.avi'\n",
        "\n",
        "files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\n",
        "files.sort()\n",
        "\n",
        "out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n",
        "[out.write(cv2.imread(fname)) for fname in files]\n",
        "out.release()"
      ],
      "metadata": {
        "id": "_SNOY8-uQ-G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_g, losses_d, real_scores, fake_scores = history"
      ],
      "metadata": {
        "id": "Ga41PHxJQ__3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses_d, '-')\n",
        "plt.plot(losses_g, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Discriminator', 'Generator'])\n",
        "plt.title('Losses');"
      ],
      "metadata": {
        "id": "1CE6P-kKRBhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(real_scores, '-')\n",
        "plt.plot(fake_scores, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.legend(['Real', 'Fake'])\n",
        "plt.title('Scores');"
      ],
      "metadata": {
        "id": "P6KESJ6QRC-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}